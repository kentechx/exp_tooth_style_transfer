# Sort out the imports!
import torch, torch.nn as nn, torchvision, torch.nn.functional as F
from torch.utils.data import DataLoader
from data.teeth import ConcatDataset
from functools import partial
from data.teeth import ImageDataset
from models.model import VAE
from argparse import ArgumentParser

import pytorch_lightning as pl


def discriminator():
    resnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=True)
    resnet = list(resnet.children())[:-1] + [nn.Flatten(), nn.Linear(512, 1, bias=True)]
    D = nn.Sequential(*resnet)
    return D


class LitVAE(pl.LightningModule):

    def __init__(self, latent_dim=128, beta=4, lr=1e-3, weight_decay=0.001, *args, **kwargs):
        super().__init__()
        self.save_hyperparameters()

        self.net = VAE(latent_dim)
        self.D = discriminator()

        vgg = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)
        self.vgg = nn.Sequential(*list(vgg.features.children())[:21])
        for params in self.vgg.parameters():
            params.requires_grad = False

        self.set_hook()

    def forward(self, x):
        return self.net(x)

    def contentLoss(self, content_emb, output_emb):
        """ Takes 2 embedding tensors generated by vgg and finds the L2 norm
        (ie. euclidan distance) between them. [See equation 12 of the Paper]"""
        return torch.linalg.norm(content_emb - output_emb, dim=[1, 2, 3]).mean()

    def styleLoss(self, style_activations, output_activations):
        """ Takes 2 lists of activation tensors hooked from vgg layers during
        forward passes using our style image and our ouput image as inputs.
        Computes the L2 norm between each of their means and standard deviations
        and returns the sum. [See equation 13 of the Paper]"""
        mu_sum = 0
        sigma_sum = 0
        for style_act, output_act in zip(style_activations, output_activations):
            mu_sum = torch.linalg.norm(mu(style_act) - mu(output_act), dim=[1, 2, 3]).mean()
            sigma_sum = torch.linalg.norm(sigma(style_act) - sigma(output_act), dim=[1, 2, 3]).mean()
        return mu_sum + sigma_sum

    def totalLoss(self, content_emb, output_emb, style_activations, output_activations):
        content_loss = self.contentLoss(content_emb, output_emb)
        style_loss = self.styleLoss(style_activations, output_activations)
        return content_loss + self.hparams.lambda_ * style_loss

    def reconstruct_loss(self, out_emb, target_emb):
        loss = [torch.linalg.norm(o - t, dim=[1, 2, 3]).mean() for o, t in zip(out_emb, target_emb)]
        return sum(loss)

    def kl_divergence(self, mu, logvar):
        return 0.5 * torch.sum(mu**2 + logvar.exp() - logvar - 1, dim=1).mean(0)

    def adversarial_loss(self, out_logits, y):
        return F.mse_loss(out_logits, y)

    def training_step(self, batch, batch_idx, optimizer_idx=0):
        x = batch
        H = self.hparams

        # train generator
        if optimizer_idx == 0:
            opt_vae = self.optimizers()[0]
            self.log('lr/opt_vae', opt_vae.param_groups[0]['lr'])

            gen_img, mu, logvar, z = self(x)
            if batch_idx % 10 == 0:
                self.x, self.gen_img = x, gen_img

            self.vgg(gen_img)
            out_emb = [act.clone() for act in self.activations]
            self.vgg(x)
            target_emb = [act.clone() for act in self.activations]

            recon_loss = self.reconstruct_loss(out_emb, target_emb)
            kld = self.kl_divergence(mu, logvar)
            l1 = F.l1_loss(gen_img, x, reduction='sum')

            valid = torch.ones(x.size(0), 1)
            valid = valid.type_as(x)  # gpu

            g_loss = self.adversarial_loss(self.D(gen_img), valid)
            self.log('g_loss', g_loss, True)
            self.log('recon_loss', recon_loss, True)
            self.log('kld', kld, True)
            # self.log('l1', l1, True)

            loss = g_loss + H.lambda_kld * kld + H.lambda_recon * recon_loss
            self.log('loss', loss)
            return loss
        #
        # # train discriminator
        if optimizer_idx == 1:
            opt_d = self.optimizers()[1]
            self.log('lr/opt_d', opt_d.param_groups[0]['lr'])
            valid = torch.ones(x.size(0), 1)
            valid = valid.type_as(x)

            real_loss = self.adversarial_loss(self.D(x), valid)

            fake = torch.zeros(x.size(0), 1)
            fake = fake.type_as(x)

            fake_loss = self.adversarial_loss(self.D(self(x)[0].detach()), fake)

            d_loss = (real_loss + fake_loss) / 2
            self.log('d_loss', d_loss, True)
            return d_loss

        # if optimizer_idx == 2:
        #     opt_g = self.optimizers()[2]
        #     self.log('lr/opt_g', opt_g.param_groups[0]['lr'])
        #     mu, logvar = self.net.encode(x)
        #     z = self.net.reparameterize(mu, logvar).detach()
        #     out = self.net.decode(z)
        #     z_out = self.net.encode(out)[0]
        #
        #     latent_loss = F.l1_loss(z_out, z, reduction='mean')
        #     self.log("latent_loss", latent_loss, True)
        #     return latent_loss

    def configure_optimizers(self):
        H = self.hparams
        opt_vae = torch.optim.Adam(self.net.parameters(), H.lr, weight_decay=H.weight_decay)
        opt_d = torch.optim.Adam(self.D.parameters(), H.lr, weight_decay=H.weight_decay)
        # opt_g = torch.optim.Adam(self.net.decoder.parameters(), H.lr, weight_decay=H.weight_decay)
        return [opt_vae, opt_d]

    def on_epoch_end(self):
        # grid = torchvision.utils.make_grid(self.c[:6])
        # self.logger.experiment.add_image('images/content', grid, self.current_epoch)
        grid = torchvision.utils.make_grid(self.x[:6])
        self.logger.experiment.add_image('images/input', grid, self.current_epoch)
        grid = torchvision.utils.make_grid(self.gen_img[:6])
        self.logger.experiment.add_image('images/generated', grid, self.current_epoch)
        self.logger.experiment.flush()

    def set_hook(self):
        self.activations = [None] * 4
        self.debug_activations = [None] * 4
        self.style_act = [None] * 4

        style_layers = ['1', '6', '11', '20']
        debug_layers = [0, 3, 5, 7]

        # declare hook function
        def recon_hook(i, module, input, output):
            self.activations[i] = output

        def style_hook(i, module, input, output):
            self.style_act[i] = output

        def debugHook(i, module, input, output):
            self.debug_activations[i] = output

        for i, layer in enumerate(style_layers):
            self.vgg._modules[layer].register_forward_hook(partial(recon_hook, i))
        # for i, layer in enumerate(style_layers):
        #     self.G.vgg._modules[layer].register_forward_hook(partial(style_hook, i))
        # for i, layer in enumerate(debug_layers):
        #     self.G.dec._modules[str(layer)].register_forward_hook(partial(debugHook, i))

    @staticmethod
    def add_model_specific_args(parent_parser):
        parser = ArgumentParser(parents=parent_parser, add_help=False)

